---
title: 【直观详解】什么是PCA、SVD
date: 2017-10-05 18:31:12
categories:
 - Machine Learning
tags:
 - Machine Learning
 - Theory
---

【阅读时间】
【内容简介】

<!-- more -->

在说明一个解释型内容的过程中，我一直坚信，**带有思考的重复的是获取的知识的唯一捷径**，所以会加入很多括号的内容，即**另一种说法（从不同角度或其他称呼等）**，这样有助于理解。加粗的地方我也认为是比较重要的关键字或者逻辑推导，学习有一个途径就是划重点，做笔记。

# What & Why PCA（主成分分析）

PCA，Principal components analyses，主成分分析。广泛应用于降维，有损数据压缩，特征提取和数据可视化。也被称为**Karhunen-Loeve变换**

从**降维的方法**角度来看，有**两种PCA的定义方式**，这里需要有一个直观的理解：什么是变换（线性代数基础），想整理一下自己线性代数的可以移步我的另一篇文章：【直观详解】线性代数的本质

- **最大化**正交投影后数据的方差（让数据在经过变换后**更加分散**）

![往低维度的投影直观表示图](【直观详解】什么是PCA、SVD/PCA.png)

> 紫色的直线 $u\_1$ 即是关于 $\{x\_1,x\_2\}$ 二维的正交投影的对应一维表示
> PCA定义为使**绿色点集的方差最小**（方差是尽量让绿色所有点都**聚在一坨**）
> 其中的蓝线是原始数据集（红点）到**低纬度的距离**，这可以引出第二种定义方式

- **最小化**投影造成的损失（下图中**所有红线（投影造成的损失）**加起来最小）

![投影造成的损失](【直观详解】什么是PCA、SVD/PCAani.gif)

SVD，Singular Value Decomposition，奇异值分解。我们知道我们可以把矩阵分解成特征向量和特征值（关于特征向量和特征值的理解详见我的另一篇博文：【直观详解】线性代数的本质）

















